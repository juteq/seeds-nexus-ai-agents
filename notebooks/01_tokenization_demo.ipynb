{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "d40d656e",
      "metadata": {},
      "source": [
       "# Tokenization Types with LangChain\n",
       "\n",
       "In this notebook, we'll use LangChain to demonstrate three common types of tokenization: word, character, and subword."
      ]
     },
     {
      "cell_type": "markdown",
      "id": "d5c430b7",
      "metadata": {},
      "source": [
       "## 1️⃣ Word Tokenization\n",
       "\n",
       "Word tokenization splits text into words. Let's see how to do this with LangChain's `RecursiveCharacterTextSplitter` (with default settings, it splits on spaces)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "67395a7c",
      "metadata": {},
      "outputs": [],
      "source": [
       "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
       "\n",
       "text = \"LangChain makes tokenization easy and flexible.\"\n",
       "\n",
       "# Word-level tokenization (split on spaces)\n",
       "word_splitter = RecursiveCharacterTextSplitter(chunk_size=1, chunk_overlap=0, separators=[\" \"])\n",
       "word_tokens = word_splitter.split_text(text)\n",
       "print(\"Word tokens:\", word_tokens)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "98487827",
      "metadata": {},
      "source": [
       "## 2️⃣ Character Tokenization\n",
       "\n",
       "Character tokenization splits text into individual characters."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "79570f35",
      "metadata": {},
      "outputs": [],
      "source": [
       "# Character-level tokenization (split on every character)\n",
       "char_splitter = RecursiveCharacterTextSplitter(chunk_size=1, chunk_overlap=0, separators=[\"\"])\n",
       "char_tokens = char_splitter.split_text(text)\n",
       "print(\"Character tokens:\", char_tokens)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "4b2f1759",
      "metadata": {},
      "source": [
       "## 3️⃣ Subword Tokenization (Simulated)\n",
       "\n",
       "LangChain does not provide built-in subword tokenization, but we can simulate it by splitting on common subword patterns (like 'ing', 'ion', etc.)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5993f1",
      "metadata": {},
      "outputs": [],
      "source": [
       "import re\n",
       "\n",
       "def simple_subword_tokenizer(text):\n",
       "    # Example: split on 'ing', 'ion', 'iz', 'ed', 'ly', 'er', 'es', 's'\n",
       "    pattern = r\"(ing|ion|iz|ed|ly|er|es|s)\"\n",
       "    tokens = re.split(pattern, text)\n",
       "    # Remove empty strings\n",
       "    return [t for t in tokens if t and not t.isspace()]\n",
       "\n",
       "subword_tokens = simple_subword_tokenizer(\"Tokenization is amazing and powerful.\")\n",
       "print(\"Subword tokens:\", subword_tokens)"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "8ceda7de",
      "metadata": {},
      "source": [
       "---\n",
       "\n",
       "**Summary:**\n",
       "- Word tokenization splits by spaces.\n",
       "- Character tokenization splits by each character.\n",
       "- Subword tokenization splits by common subword patterns (simulated here).\n",
       "\n",
       "You can use LangChain's text splitters for flexible tokenization in your NLP workflows!"
      ]
     }
    ],
    "metadata": {
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }

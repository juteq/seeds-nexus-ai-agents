{
    "cells": [
      {
        "cell_type": "markdown",
        "id": "header",
        "metadata": {},
        "source": [
          "# Prompt Engineering Examples for Presentation\n",
          "## Foundations of Generative AI - Module 1\n",
          "\n",
          "This notebook demonstrates the 5 key prompt engineering concepts from the slides:\n",
          "1. **Clarity and Specificity**\n",
          "2. **Contextual Information** \n",
          "3. **Role and Tone**\n",
          "4. **Structured Output**\n",
          "5. **Token Management**"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "setup",
        "metadata": {},
        "outputs": [],
        "source": [
          "# Installation and Setup\n",
          "!pip install openai python-dotenv -q\n",
          "\n",
          "import os\n",
          "from openai import OpenAI\n",
          "from getpass import getpass\n",
          "\n",
          "# Set your OpenAI API key here\n",
          "try:\n",
          "    import google.colab\n",
          "    IN_COLAB = True\n",
          "except ImportError:\n",
          "    IN_COLAB = False\n",
          "\n",
          "# Set up OpenAI API key based on environment\n",
          "if IN_COLAB:\n",
          "    # For Google Colab: use the secure input method\n",
          "    from google.colab import userdata\n",
          "\n",
          "    try:\n",
          "        # Try to get API key from Colab secrets first\n",
          "        openai_api_key = userdata.get('OPENAI_API_KEY')\n",
          "        if openai_api_key:\n",
          "            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
          "            print(\"‚úÖ API key loaded from Google Colab secrets!\")\n",
          "        else:\n",
          "            # If not in secrets, prompt user to enter it\n",
          "            from getpass import getpass\n",
          "            print(\"OpenAI API key not found in Colab secrets.\")\n",
          "            os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
          "            print(\"‚úÖ API key set from input\")\n",
          "    except Exception as e:\n",
          "        print(f\"Note: {e}\")\n",
          "        print(\"Enter your OpenAI API key below:\")\n",
          "        os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key: \")\n",
          "else:\n",
          "    # For local environment: try to load from .env file\n",
          "    try:\n",
          "        from dotenv import load_dotenv\n",
          "        load_dotenv()\n",
          "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
          "        if api_key:\n",
          "            print(\"‚úÖ API key loaded from .env file\")\n",
          "        else:\n",
          "            print(\"‚ö†Ô∏è No API key found in .env file. Using fallback value.\")\n",
          "            os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
          "    except ImportError:\n",
          "        print(\"‚ö†Ô∏è python-dotenv not installed. Using fallback value.\")\n",
          "        os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
          "\n",
          "# Check if API key is properly set\n",
          "if os.environ.get(\"OPENAI_API_KEY\") in [None, \"\", \"your-api-key-here\"]:\n",
          "    print(\"‚ö†Ô∏è WARNING: Please set your OpenAI API key before running the examples!\")\n",
          "    print(\"Get your API key from: https://platform.openai.com/api-keys\")\n",
          "    if IN_COLAB:\n",
          "        print(\"For Colab: Use Secrets to securely store your API key\")\n",
          "        print(\"  1. Click on the üîë icon in the left sidebar\")\n",
          "        print(\"  2. Add a new secret with name 'OPENAI_API_KEY'\")\n",
          "        print(\"  3. Run this cell again\")\n",
          "    else:\n",
          "        print(\"For local use: Create a .env file with OPENAI_API_KEY=your-key-here\")\n",
          "else:\n",
          "    print(\"‚úÖ API key is set! Ready to proceed.\\n\")\n",
          "\n",
          "print(\"LangChain Tutorial: 5 Essential Concepts\")\n",
          "print(\"=\" * 60)\n",
          "\n",
          "# Initialize OpenAI client\n",
          "client = OpenAI()"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "helper-function",
        "metadata": {},
        "outputs": [],
        "source": [
          "# Helper function to call OpenAI API\n",
          "def get_completion(messages, model=\"gpt-4o-mini\", max_tokens=150, temperature=0.7):\n",
          "    \"\"\"Get completion from OpenAI API\"\"\"\n",
          "    try:\n",
          "        response = client.chat.completions.create(\n",
          "            model=model,\n",
          "            messages=messages,\n",
          "            max_tokens=max_tokens,\n",
          "            temperature=temperature\n",
          "        )\n",
          "        return response.choices[0].message.content\n",
          "    except Exception as e:\n",
          "        return f\"Error: {e}. Please check your API key and try again.\"\n",
          "\n",
          "print(\"‚úÖ Helper function ready!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "clarity-header",
        "metadata": {},
        "source": [
          "## 1. Clarity and Specificity\n",
          "Clear instructions with specific details lead to better responses"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "clarity-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"üéØ CLARITY AND SPECIFICITY\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "# Vague prompt\n",
          "vague_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"Tell me about programming\"}\n",
          "]\n",
          "\n",
          "# Clear and specific prompt\n",
          "clear_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"\"\"\n",
          "Write a beginner-friendly explanation of Python programming that includes:\n",
          "1. What Python is used for (3 examples)\n",
          "2. Why it's good for beginners (2 reasons)\n",
          "3. One simple code example with explanation\n",
          "\n",
          "Format: 150 words maximum, use bullet points\n",
          "Tone: Encouraging and accessible\n",
          "\"\"\"}\n",
          "]\n",
          "\n",
          "print(\"‚ùå VAGUE PROMPT:\")\n",
          "print(f\"Prompt: {vague_messages[0]['content']}\")\n",
          "vague_response = get_completion(vague_messages)\n",
          "print(f\"Response: {vague_response}\")\n",
          "print(f\"Response length: {len(vague_response)} characters\\n\")\n",
          "\n",
          "print(\"‚úÖ CLEAR & SPECIFIC PROMPT:\")\n",
          "print(f\"Prompt: {clear_messages[0]['content']}\")\n",
          "clear_response = get_completion(clear_messages)\n",
          "print(f\"Response: {clear_response}\")\n",
          "print(f\"Response length: {len(clear_response)} characters\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "context-header",
        "metadata": {},
        "source": [
          "## 2. Contextual Information\n",
          "Providing background information and examples improves response quality"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "context-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"üîç CONTEXTUAL INFORMATION\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "# Without context\n",
          "no_context = [\n",
          "    {\"role\": \"user\", \"content\": \"Should I invest in this?\"}\n",
          "]\n",
          "\n",
          "# With rich context\n",
          "with_context = [\n",
          "    {\"role\": \"user\", \"content\": \"\"\"\n",
          "Context: I'm a 28-year-old software engineer with:\n",
          "- $50k in savings\n",
          "- Stable job ($80k/year)\n",
          "- No debt\n",
          "- Goal: Buy a house in 3-5 years\n",
          "\n",
          "Investment opportunity: Tech startup offering 15% equity for $25k investment.\n",
          "Startup details: AI productivity tool, 2 years old, $500k revenue last year.\n",
          "\n",
          "Question: Should I invest in this startup given my financial situation and goals?\n",
          "\"\"\"}\n",
          "]\n",
          "\n",
          "print(\"‚ùå NO CONTEXT:\")\n",
          "no_context_response = get_completion(no_context)\n",
          "print(f\"Response: {no_context_response}\\n\")\n",
          "\n",
          "print(\"‚úÖ WITH CONTEXT:\")\n",
          "context_response = get_completion(with_context)\n",
          "print(f\"Response: {context_response}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "few-shot-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\nüìö FEW-SHOT LEARNING EXAMPLE:\")\n",
          "print(\"=\" * 35)\n",
          "\n",
          "# Few-shot example\n",
          "few_shot_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"\"\"\n",
          "Classify these customer emails as: Happy, Angry, or Neutral\n",
          "\n",
          "Examples:\n",
          "Email: \"Thank you so much! This product changed my life!\"\n",
          "Classification: Happy\n",
          "\n",
          "Email: \"This is completely broken and your support is terrible!\"\n",
          "Classification: Angry\n",
          "\n",
          "Email: \"When will my order arrive? I placed it last week.\"\n",
          "Classification: Neutral\n",
          "\n",
          "Now classify this:\n",
          "Email: \"I love the new features you added! The interface is so much better now.\"\n",
          "Classification:\n",
          "\"\"\"}\n",
          "]\n",
          "\n",
          "few_shot_response = get_completion(few_shot_messages, max_tokens=50)\n",
          "print(f\"Response: {few_shot_response}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "role-header",
        "metadata": {},
        "source": [
          "## 3. Role and Tone\n",
          "Assigning specific roles and tones shapes the response style"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "role-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"üé≠ ROLE AND TONE\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "# Math Tutor Role\n",
          "math_tutor_messages = [\n",
          "    {\"role\": \"system\", \"content\": \"You are a helpful math tutor for middle school students. Explain concepts clearly and encouragingly.\"},\n",
          "    {\"role\": \"user\", \"content\": \"What is the Pythagorean theorem?\"}\n",
          "]\n",
          "\n",
          "# Customer Service Role\n",
          "customer_service_messages = [\n",
          "    {\"role\": \"system\", \"content\": \"You are a professional customer service representative. Be helpful, polite, and solution-focused.\"},\n",
          "    {\"role\": \"user\", \"content\": \"My order hasn't arrived and I'm frustrated.\"}\n",
          "]\n",
          "\n",
          "print(\"üë®‚Äçüè´ MATH TUTOR ROLE:\")\n",
          "math_response = get_completion(math_tutor_messages)\n",
          "print(f\"Response: {math_response}\\n\")\n",
          "\n",
          "print(\"üéß CUSTOMER SERVICE ROLE:\")\n",
          "service_response = get_completion(customer_service_messages)\n",
          "print(f\"Response: {service_response}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "tone-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "# Shakespeare Style\n",
          "shakespeare_messages = [\n",
          "    {\"role\": \"system\", \"content\": \"You are Shakespeare. Respond in Elizabethan English with poetic flair.\"},\n",
          "    {\"role\": \"user\", \"content\": \"What is love?\"}\n",
          "]\n",
          "\n",
          "# Casual Friend Style\n",
          "casual_messages = [\n",
          "    {\"role\": \"system\", \"content\": \"You are a casual, friendly buddy. Use informal language and be enthusiastic.\"},\n",
          "    {\"role\": \"user\", \"content\": \"I got a promotion at work!\"}\n",
          "]\n",
          "\n",
          "print(\"üé≠ SHAKESPEARE STYLE:\")\n",
          "shakespeare_response = get_completion(shakespeare_messages)\n",
          "print(f\"Response: {shakespeare_response}\\n\")\n",
          "\n",
          "print(\"üòÑ CASUAL FRIEND STYLE:\")\n",
          "casual_response = get_completion(casual_messages)\n",
          "print(f\"Response: {casual_response}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "structured-header",
        "metadata": {},
        "source": [
          "## 4. Structured Output\n",
          "Requesting specific formats makes responses easier to parse and use"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "structured-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"üìã STRUCTURED OUTPUT\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "# JSON format request\n",
          "json_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"\"\"\n",
          "Extract information from this text and return it in JSON format:\n",
          "\n",
          "\"John Smith is a 30-year-old software engineer from New York. He specializes in Python and machine learning. He has worked at Google for 5 years and holds a Master's degree in Computer Science from MIT.\"\n",
          "\n",
          "Required JSON structure:\n",
          "{\n",
          "  \"name\": \"\",\n",
          "  \"age\": 0,\n",
          "  \"profession\": \"\",\n",
          "  \"location\": \"\",\n",
          "  \"skills\": [],\n",
          "  \"experience\": {\n",
          "    \"company\": \"\",\n",
          "    \"years\": 0\n",
          "  },\n",
          "  \"education\": {\n",
          "    \"degree\": \"\",\n",
          "    \"institution\": \"\"\n",
          "  }\n",
          "}\n",
          "\"\"\"}\n",
          "]\n",
          "\n",
          "print(\"üìÑ JSON FORMAT REQUEST:\")\n",
          "json_response = get_completion(json_messages)\n",
          "print(f\"Response: {json_response}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "table-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "# Markdown table request\n",
          "table_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"\"\"\n",
          "Create a comparison table in Markdown format for these programming languages:\n",
          "\n",
          "Python: Easy to learn, Great for AI/ML, Slower execution\n",
          "JavaScript: Web development, Large ecosystem, Flexible syntax\n",
          "Java: Enterprise applications, Strong typing, Platform independent\n",
          "\n",
          "Format as a Markdown table with columns: Language, Strengths, Primary Use Cases\n",
          "\"\"\"}\n",
          "]\n",
          "\n",
          "print(\"\\nüìä MARKDOWN TABLE REQUEST:\")\n",
          "table_response = get_completion(table_messages)\n",
          "print(f\"Response: {table_response}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "token-header",
        "metadata": {},
        "source": [
          "## 5. Token Management\n",
          "Efficient prompts and token limits control response length and cost"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "token-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"‚ö° TOKEN MANAGEMENT\")\n",
          "print(\"=\" * 40)\n",
          "\n",
          "# Inefficient prompt (verbose)\n",
          "inefficient_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"\"\"\n",
          "I would like to request that you please provide me with a comprehensive and detailed explanation regarding the fundamental concepts and principles of machine learning, including but not limited to the various types of algorithms, methodologies, and approaches that are commonly used in this field, as well as their respective applications and use cases in real-world scenarios.\n",
          "\"\"\"}\n",
          "]\n",
          "\n",
          "# Efficient prompt (concise)\n",
          "efficient_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"\"\"\n",
          "Explain machine learning fundamentals:\n",
          "- Main algorithm types\n",
          "- Key applications\n",
          "- Real-world examples\n",
          "Keep response under 100 words.\n",
          "\"\"\"}\n",
          "]\n",
          "\n",
          "print(\"‚ùå INEFFICIENT (Verbose):\")\n",
          "print(f\"Prompt length: {len(inefficient_messages[0]['content'])} characters\")\n",
          "inefficient_response = get_completion(inefficient_messages, max_tokens=150)\n",
          "print(f\"Response: {inefficient_response}\\n\")\n",
          "\n",
          "print(\"‚úÖ EFFICIENT (Concise):\")\n",
          "print(f\"Prompt length: {len(efficient_messages[0]['content'])} characters\")\n",
          "efficient_response = get_completion(efficient_messages, max_tokens=150)\n",
          "print(f\"Response: {efficient_response}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "token-control",
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"\\n‚ö° TOKEN CONTROL DEMONSTRATION:\")\n",
          "print(\"=\" * 35)\n",
          "\n",
          "# Token limit demonstration\n",
          "short_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"Explain AI in exactly 25 words.\"}\n",
          "]\n",
          "\n",
          "medium_messages = [\n",
          "    {\"role\": \"user\", \"content\": \"Explain AI in exactly 100 words.\"}\n",
          "]\n",
          "\n",
          "print(\"üî∏ 25 WORD LIMIT:\")\n",
          "short_response = get_completion(short_messages, max_tokens=40)\n",
          "print(f\"Response: {short_response}\")\n",
          "print(f\"Word count: {len(short_response.split())} words\\n\")\n",
          "\n",
          "print(\"üî∏ 100 WORD LIMIT:\")\n",
          "medium_response = get_completion(medium_messages, max_tokens=150)\n",
          "print(f\"Response: {medium_response}\")\n",
          "print(f\"Word count: {len(medium_response.split())} words\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "complete-header",
        "metadata": {},
        "source": [
          "## Complete Example - All Techniques Combined\n",
          "This example demonstrates all 5 prompt engineering principles working together"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "complete-demo",
        "metadata": {},
        "outputs": [],
        "source": [
          "print(\"üéØ COMPLETE EXAMPLE - ALL TECHNIQUES COMBINED\")\n",
          "print(\"=\" * 50)\n",
          "\n",
          "# Perfect prompt incorporating all principles\n",
          "perfect_messages = [\n",
          "    {\n",
          "        \"role\": \"system\", \n",
          "        \"content\": \"You are an expert data scientist and business consultant. Provide clear, actionable advice with specific examples.\"\n",
          "    },\n",
          "    {\n",
          "        \"role\": \"user\", \n",
          "        \"content\": \"\"\"\n",
          "Context: I'm a small retail business owner with 3 stores, 50 employees, and $2M annual revenue. I have basic POS systems but no advanced analytics.\n",
          "\n",
          "Task: Recommend 3 AI/ML solutions for my business.\n",
          "\n",
          "Requirements:\n",
          "- Focus on practical, implementable solutions\n",
          "- Include estimated costs and timeframes\n",
          "- Explain ROI potential\n",
          "- Consider my business size and resources\n",
          "\n",
          "Format your response as JSON:\n",
          "{\n",
          "  \"recommendations\": [\n",
          "    {\n",
          "      \"solution\": \"\",\n",
          "      \"description\": \"\",\n",
          "      \"cost_estimate\": \"\",\n",
          "      \"timeframe\": \"\",\n",
          "      \"roi_potential\": \"\",\n",
          "      \"implementation_difficulty\": \"\"\n",
          "    }\n",
          "  ]\n",
          "}\n",
          "\n",
          "Maximum response: 300 words\n",
          "\"\"\"\n",
          "    }\n",
          "]\n",
          "\n",
          "print(\"üéØ PERFECT PROMPT ANALYSIS:\")\n",
          "print(\"‚úÖ Clear role assignment (system message)\")\n",
          "print(\"‚úÖ Rich context provided (business details)\")\n",
          "print(\"‚úÖ Specific requirements (practical focus)\")\n",
          "print(\"‚úÖ Structured output format (JSON)\")\n",
          "print(\"‚úÖ Token limit specified (300 words)\\n\")\n",
          "\n",
          "perfect_response = get_completion(perfect_messages, max_tokens=400)\n",
          "print(f\"Response: {perfect_response}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "summary-header",
        "metadata": {},
        "source": [
          "## Summary: Prompt Engineering Best Practices\n",
          "\n",
          "### Key Takeaways:\n",
          "1. **Be Specific**: Clear instructions with specific details get better results\n",
          "2. **Provide Context**: Background information and examples improve accuracy\n",
          "3. **Define Roles**: System messages shape the AI's personality and expertise\n",
          "4. **Structure Output**: Request specific formats for easier parsing\n",
          "5. **Manage Tokens**: Control length and cost with efficient prompts\n",
          "\n",
          "### Next Steps:\n",
          "- Experiment with different combinations of these techniques\n",
          "- Test prompts with various models and compare results\n",
          "- Build these concepts into your LangChain applications\n"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.8.5"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 5
  }